I"£L<h3 id="course-statistical-machine-learning">Course: Statistical Machine Learning</h3>
<h3 id="professor-alicia-johnson">Professor: Alicia Johnson</h3>
<h3 id="collaborators-anael-kuperwajs-cohen">Collaborators: Anael Kuperwajs Cohen</h3>

<p><strong>Prompt:</strong></p>

<h2 id="part-1-process-the-data">Part 1: Process the data</h2>

<pre><code class="language-{r}">buzzfeed &lt;- read.csv("https://www.macalester.edu/~ajohns24/data/buzzfeed.csv")

buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(title = as.character(title),
         text = as.character(text),
         url = as.character(url))
</code></pre>

<h3 id="new-predictors">New predictors</h3>

<p>Our new predictors will be:</p>

<ul>
  <li>Word count</li>
  <li>Word count in title</li>
  <li>Upper-case word count</li>
  <li>Upper-case word count in title</li>
  <li>Upper-case word ratio in title</li>
  <li>! ratio (to . ?)</li>
  <li>! ratio (to . ?) in title</li>
  <li>Sentence Count</li>
  <li>Syllables</li>
  <li>% Unique words</li>
  <li>Flesch Reading Ease</li>
  <li>Author Count</li>
  <li>Primary sentiment</li>
  <li>Secondary sentiment</li>
  <li>Strength of primary sentiment</li>
  <li>Strength of secondary sentiment</li>
</ul>

<p>Making <code class="highlighter-rouge">total_words</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(total_words = str_count(text, " ") + 1)
</code></pre>

<p>Making <code class="highlighter-rouge">total_words_title</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(total_words_title = str_count(title, " ") + 1)
</code></pre>

<p>Making <code class="highlighter-rouge">total_upper_case_words</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(total_upper_case_words = str_count(text, "\\b[A-Z]{2,}\\b"))
</code></pre>

<p>Making <code class="highlighter-rouge">total_upper_case_words_title</code> and <code class="highlighter-rouge">upper_case_word_ratio_title</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(total_upper_case_words_title = str_count(title, "\\b[A-Z]{2,}\\b"),
         upper_case_word_ratio_title = total_upper_case_words_title/total_words_title)
</code></pre>

<p>Making <code class="highlighter-rouge">exclamation_ratio</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(exclamation_ratio = exclamation_ratio(text))
</code></pre>

<p>Making <code class="highlighter-rouge">exclamation_ratio_title</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(exclamation_ratio_title = exclamation_ratio(title))
</code></pre>

<p>Making <code class="highlighter-rouge">total_sentences</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(total_sentences = nsentence(text))
</code></pre>

<p>Making <code class="highlighter-rouge">total_syllables</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(total_syllables = nsyllable(text))
</code></pre>

<p>Making <code class="highlighter-rouge">unique_word_percent</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(unique_word_percent = pct_unique_words(text, total_words))
</code></pre>

<p>Making <code class="highlighter-rouge">readability_score</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;% 
  mutate(readability_score = flesch_reading_ease(total_words, total_sentences, total_syllables))
</code></pre>

<p>Making <code class="highlighter-rouge">author_count</code></p>
<pre><code class="language-{r}">buzzfeed &lt;- buzzfeed %&gt;%
  mutate(authors = gsub("View All Posts,", "", authors)) %&gt;%
  mutate(authors = gsub("View All Posts", "", authors)) %&gt;%
  mutate(authors = gsub("Abc News,", "", authors)) %&gt;%
  mutate(authors = gsub("Abc News", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn National Politics Reporter,", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn National Politics Reporter", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn Pentagon Correspondent,", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn Pentagon Correspondent", "", authors)) %&gt;%
  mutate(authors = gsub("Latest Posts,", "", authors)) %&gt;%
  mutate(authors = gsub("Latest Posts", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn White House Producer,", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn White House Producer", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn Senior Congressional Producer,", "", authors)) %&gt;%
  mutate(authors = gsub("Cnn Senior Congressional Producer", "", authors)) %&gt;%
  mutate(author_count = str_count(authors, ",") + 1) %&gt;% 
  mutate(author_count = if_else(author_count == 0, 1, author_count)) %&gt;% 
  mutate(authors = if_else(authors == "", "None", authors)) %&gt;% 
  mutate(author_count = if_else(authors == "None", 0, author_count))
</code></pre>

<p>Making <code class="highlighter-rouge">primary_sentiment</code>, <code class="highlighter-rouge">secondary_sentiment</code>, <code class="highlighter-rouge">primary_sentiment_value</code>, and <code class="highlighter-rouge">secondary_sentiment_value</code>
```{r cache=TRUE}
buzzfeed &lt;- buzzfeed %&gt;% 
  rowwise() %&gt;% 
  mutate(primary_sentiment = max_sentiment_type(text, 1),
         primary_sentiment_value = max_sentiment_value(text, 1),
         secondary_sentiment = max_sentiment_type(text, 2),
         secondary_sentiment_value = max_sentiment_value(text, 2)
         )</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

```{r echo = TRUE}
buzzfeed[c(8,153),] %&gt;% 
  select(-text) %&gt;% 
  kable() %&gt;%
  kable_styling() %&gt;%
  scroll_box(width = "100%", height = "200px")
</code></pre></div></div>

<p><br />
<br />
\</p>

<p>The two articles that will be presented in order to demonstrate our new predictors are ‚ÄúJeb Bush to lecture at Harvard this fall‚Äù ‚Äì the real sample article ‚Äì and ‚ÄúHillary‚Äôs DEAD!?!? Brand New Theory Has Serious PROOF‚Äù ‚Äì  the fake article. 
We first started by looking at predictors relating to word and punctuation counts in the text itself and the title. Our first predictors looked at the word count and upper-case word count in the text and the title. The total number of words for the title and the text were not significantly different between these two articles. The number of words in the titles is exactly the same, and the word count in the body only differs by 50.</p>

<p>The total number of upper-case words in the text is also similar in both articles: one in the real article and two in the fake. However, the difference in the number of upper-case words is more drastic in the title. There are zero upper-case words in the real title whereas two out of eight words in the fake title are uppercase. We also capture this as a ratio. For the fake title, two out of eight words is 25%, as seen in the predictor <code class="highlighter-rouge">total_upper_case_words_title</code>.</p>

<p>The next two predictors looked at the ratio of exclamation points to question marks and periods in the text and title. In the text, there are zero exclamation points in real article, yet about 47% of sentence ending punctuation (.?!) in the fake article is made up of exclamations. Furthermore, in the titles, there are zero exclamation points in the real article but they make up 50% of the punctuation for the fake article.</p>

<p>Just like word count, we also looked at sentence and syllable count. These two measurements were fairly insignificant as the numbers were similar in both sentences but were useful in the computation of other new predictors. It was slightly surprising, however, that there were more sentences and syllables in the fake news article than the real one.</p>

<p>The next two predictors analyzed the words that were used in the text. The first is the percent of unique words, which was much higher for the real article, 64% vs 76%. The second is a Flesch Reading Ease score, where higher score implies the text is easier to read. Texts with high scores can be read by a younger audience. The real article has a readability score of 23.34, which implies a reading level of a college <em>graduate</em>. On the other hand, the fake article has a score of 48.71, which implies a reading level of a college <em>student</em>. While this may not seem like a huge difference, the idea is that the structure of the article is more complex when the score is lower. Fake articles often don‚Äôt have that same complexity.</p>

<p>The next predictor is about the author count, which is equal in this case between the two articles. It does not make a big difference in this comparison.</p>

<p>Finally, the last four predictors are about sentiment analysis. We looked at the primary and secondary sentiment in the text and the strength of each sentiment. The primary sentiment is the strongest sentiment in the text and the second sentiment is the second strongest where strength is calculated by total sentences containing that sentiment. The sentiments for the real article are <code class="highlighter-rouge">trust</code> and <code class="highlighter-rouge">positive</code>, with scores in the 30s, while the fake article has the sentiments <code class="highlighter-rouge">negative</code> and <code class="highlighter-rouge">positive</code> with low scores of 16 each. The fact that the primary and secondary sentiments conflict with each other greatly is something to be concerned about when thinking of the validity of an article.</p>

<p>Similar to any analytical method, text analysis has it‚Äôs drawbacks. One drawback, for example, is seen through the sentiment analysis predictors. We will not always be able to accurately analyze the sentiment of text. Computers and algorithms can‚Äôt always detect sarcasm, understand references or allusions, and won‚Äôt always correctly analyze basic sentiments such as <code class="highlighter-rouge">positive</code> or <code class="highlighter-rouge">negative</code>. Additionally, writing algorithms to detect factors like writing style, words with multiple meanings, correct grammar/puntuation, and correctness of fact is often difficult, innacurate, or impossible.  All of that being said, text analysis is still a beneficial and important research tool.</p>

<p><br />
<br />
<br />
<br />
<br />
\</p>

<h2 id="part-2-analyze">Part 2: Analyze</h2>

<p>LASSO with source</p>

<pre><code class="language-{r}">set.seed(253)
lambda_grid &lt;- 10^seq(-3, 1, length = 100)

lasso_model &lt;- train(
    type ~ .,
    data = buzzfeed %&gt;% select(-text, -title, -url),
    method = "glmnet",
    tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
    metric = "Accuracy",
    na.action = na.omit
)
</code></pre>

<p>LASSO without source</p>

<pre><code class="language-{r}">set.seed(253)
lambda_grid &lt;- 10^seq(-3, 1, length = 100)

lasso_model_no_source &lt;- train(
    type ~ .,
    data = buzzfeed %&gt;% select(-text, -title, -url, -source),
    method = "glmnet",
    tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
    metric = "Accuracy",
    na.action = na.omit
)
</code></pre>

<p>To build our model, we first looked at the relationships between predictors and types of article. We then tried a variety of techniques to get a sense of which might do the best job of predicting if an article is real or fake.  We tried KNN and GAM, but GAM did not do well with so much text and KNN does not do so well with so many predictors, so we moved on to parametric algorithms.</p>

<p>We chose LASSO with binary classification because simple logistic regression tried to use all of the predictors with no penalties and thus did not do as good of a job making accurate predictions. We did not try backwards stepwise or best subset selection because they are more computationally expensive.  LASSO had a very high accuracy and high specificity so we moved forward with it.</p>

<p>From here we made two models ‚Äì one that includes <code class="highlighter-rouge">source</code> and one that does not. We did this because source significantly affects accuracy.  If we were to try classifying an article with no source, the accuracy would be lower and the model would be different. When training our model, we excluded <code class="highlighter-rouge">text</code>, <code class="highlighter-rouge">title</code>, and <code class="highlighter-rouge">url</code> from the data because they are unique to each article and therefore do not make useful predictors.</p>

<p>For our models, we used a broad range of lambda ($\lambda$) values to get a better understanding of what size of penalty yielded the highest accuracy.  We chose the selection function <code class="highlighter-rouge">best</code> with the metric of <code class="highlighter-rouge">Accuracy</code>, because for a topic like fake news with potentialy impactful consequences, it is important to be as accurate as possible.</p>

<p>LASSO $\lambda$ with source</p>
<pre><code class="language-{r}">plot(lasso_model, xlim=c(0, .3))
</code></pre>

<p>LASSO $\lambda$ without source</p>
<pre><code class="language-{r}">plot(lasso_model_no_source, xlim=c(0, .3))
</code></pre>

<p><br />
<br />
<br />
<br />
<br />
\</p>

<h2 id="part-3-summarize">Part 3: Summarize</h2>

<p>After analyzing our data and looking at the results, we can conclude that some of the best predictors for our model included source, exclamation ratio in the text and title, authors, and surprise sentiment. Some of the worst predictors included the four sentiment predictors overall: primary and secondary sentiment and sentiment strength. The predictors that did not lean in either direction include <code class="highlighter-rouge">readability_score</code>, <code class="highlighter-rouge">unique_word_percent</code>, and <code class="highlighter-rouge">author_count</code> because they weren‚Äôt disjoint but also not completely intersectional between real and fake articles. We did, however, have two different models, one with source and one without source as a predictor. Both are very accurate, but when taking <code class="highlighter-rouge">source</code> out as a predictor, sentiment becomes a better predictor.</p>

<pre><code class="language-{r}">buzzfeed %&gt;% 
  ggplot(aes(x = source, fill= type))+
  geom_bar()+ 
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(breaks = scales::pretty_breaks(10))+
  labs(x="Source", y="Count",  fill= "Type", title="Count of Real vs Fake articles by source")
</code></pre>

<p>We can see from the plot above that there is very little overlap of real and fake news for each source. This makes it an extremely good predictor.</p>

<p>We can also see that the exclamation ratios are useful predictors because they have little overlap across real and fake articles.</p>

<p>```{r warning=FALSE}
eratio &lt;- buzzfeed %&gt;% 
  ggplot(aes(x=exclamation_ratio, fill=type))+
  geom_density(alpha=0.5)+
  labs(x=‚ÄùExclamation Ratio in Body‚Äù, fill=‚ÄùType‚Äù, y = ‚ÄúDensity‚Äù)+
  theme_minimal()</p>

<p>eration_title &lt;- buzzfeed %&gt;% 
  ggplot(aes(x=exclamation_ratio_title, fill=type))+
  geom_density(alpha=0.5)+
  labs(x=‚ÄùExclamation Ratio in Title‚Äù, fill=‚ÄùType‚Äù, y = ‚ÄúDensity‚Äù)+
  theme_minimal()</p>

<p>gridExtra::grid.arrange(eratio, eration_title)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
When looking at sentiment analysis, we see that the strength and type of primary sentiment are not very useful *except* in the case of surprise.  Surprise is only encountered in real articles, making it a good predictor of article type. As mentioned before, this fact changes if `source` is removed as a predictor. When that happens, sentiment becomes an important predictor.

```{r, warning=FALSE}
buzzfeed %&gt;% 
  ggplot(aes(x=primary_sentiment_value, fill = type))+
  geom_density(alpha = 0.5)+
  facet_wrap(~primary_sentiment)+
  xlim(0,60)+
  theme_minimal()+
  labs(x = "Primary Sentiment Value", y = "Density", fill = "Type")+
  theme(legend.direction = 'horizontal', legend.position=c(.8,.2))
  

buzzfeed %&gt;% 
  ggplot(aes(x=type, fill = type))+
  geom_bar()+
  facet_grid(~primary_sentiment)+
  theme_minimal()+
  labs(x = "Type", y = "Count", title = "Primary Sentiment Type")+
  theme(legend.position = "none")+
  scale_y_continuous(breaks = scales::pretty_breaks())
</code></pre></div></div>

<p>Overall, <code class="highlighter-rouge">source</code> and <code class="highlighter-rouge">author</code> are two of the most indicative predictors. From the predictors we created, the exclamation ratio in both the text and the title play an important role in categorizing the article. When <code class="highlighter-rouge">source</code> is removed, sentiment analysis also becomes important as a predictor. The LASSO model we created from these predictors was highly accurate with or without <code class="highlighter-rouge">source</code> as a predictor and the specificty was extremely high. With <code class="highlighter-rouge">source</code> as a predictor, specificity is at 100%. Without <code class="highlighter-rouge">source</code> as a predictor, specificity is approximately 97%. In both cases, specificity is high, which is very important for this task. For categorizing an article as fake or real news, it is more important to have a high specificity because it is better to accurately know which articles are fake, rather than accidentally categorize an article as real news. Lastly, the sensitivity is also high (77% and 90%) which means our models also accuratly classify articles as real. Overall, as seen from our predictors and the sensitivity and specificty values, it is easier to categorize a fake article than a real article, yet both are easily detectable.</p>

<p><br />
<br />
<br />
<br />
<br />
\</p>

<h2 id="part-4-contributions">Part 4: Contributions</h2>

<p>Both members evenly contributed to this project. We came up with the new predictors together and each implemented half of them. The rest was done together.</p>

<p><br />
<br />
\</p>

<h2 id="appendix">Appendix</h2>

<p><br />
<br />
\</p>

<h4 id="lasso-model-summary-data-with-source">LASSO model summary data with <code class="highlighter-rouge">source</code></h4>

<pre><code class="language-{r}">lasso_model$results %&gt;% filter(lambda == lasso_model$bestTune$lambda)
coef(lasso_model$finalModel, lasso_model$bestTune$lambda)


predict_data &lt;- na.omit(lasso_model$trainingData)
classifications &lt;- predict(lasso_model, newdata = predict_data, type = "raw")
head(classifications, 3)


confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
</code></pre>

<p><br />
<br />
\</p>

<h4 id="lasso-model-summary-data-without-source">LASSO model summary data without <code class="highlighter-rouge">source</code></h4>

<pre><code class="language-{r}">lasso_model_no_source$results %&gt;% filter(lambda == lasso_model_no_source$bestTune$lambda)
coef(lasso_model_no_source$finalModel, lasso_model_no_source$bestTune$lambda)


predict_data_no_source &lt;- na.omit(lasso_model_no_source$trainingData)
classifications &lt;- predict(lasso_model_no_source, newdata = predict_data, type = "raw")
head(classifications, 3)


confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
</code></pre>

:ET